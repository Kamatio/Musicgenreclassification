training_epochs = 1000
n_dim = training_features.shape[1]
n_classes = 2
n_hidden_units_one = 300
n_hidden_units_two = 800

sd = 1 / np.sqrt(n_dim)
learning_rate = 0.001

X = tf.placeholder(tf.float32,[None,n_dim])
Y = tf.placeholder(tf.float32,[None,n_classes])

Weights_1 = tf.Variable(tf.random_normal([n_dim,n_hidden_units_one]), name = 'w1')
bias_1 = tf.Variable(tf.random_normal([n_hidden_units_one]), name = 'b1')
activation_1 = tf.nn.relu(tf.matmul(X , Weights_1) + bias_1)

Weights_2 = tf.Variable(tf.random_normal([n_hidden_units_one,n_hidden_units_two]), name = 'w2')
bias_2 = tf.Variable(tf.random_normal([n_hidden_units_two]), name = 'b2')
activation_2 = tf.nn.sigmoid(tf.matmul(activation_1,Weights_2) + bias_2)

Weights = tf.Variable(tf.random_normal([n_hidden_units_two,n_classes]), name = 'W')
bias = tf.Variable(tf.random_normal([n_classes]), name = 'B')
yPredbyNN = tf.nn.softmax(tf.matmul(activation_2,Weights) + bias)

tf.add_to_collection('vars', Weights_1)
tf.add_to_collection('vars', bias_1)

tf.add_to_collection('vars', Weights_2)
tf.add_to_collection('vars', bias_2)

tf.add_to_collection('vars', Weights)
tf.add_to_collection('vars', bias)

saver = tf.train.Saver()

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(yPredbyNN, Y))
#optimizer = tf.train.RMSPropOptimizer(learning_rate, decay = 0.75).minimize(loss)
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

CorrectPred = tf.equal(tf.argmax(yPredbyNN,1), tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(CorrectPred, tf.float32))

init = tf.global_variables_initializer()

Loss_data = np.empty(shape=[1],dtype=float)
y_true, y_pred = None, None

ClassLabelFinal = []

with tf.Session() as sess:
    sess.run(init)
    for epoch in range(training_epochs):
        _,cost = sess.run([optimizer,loss],feed_dict={X: training_features,Y:training_labels})
        Loss_data = np.append(Loss_data,cost)

    y_pred = sess.run(tf.argmax(yPredbyNN,1),feed_dict={X: testing_features})
    y_true = sess.run(tf.argmax(testing_labels,1))
    print('Test accuracy: ',round(sess.run(accuracy, feed_dict={X: testing_features, Y: testing_labels}), 2))
    
    
    saver.save(sess, '/Users/MA/Desktop/Mymodel')
    
    
  #ACCURACYPLOT  
    
 from sklearn.metrics import precision_recall_fscore_support

fig = plt.figure(figsize=(10,8))
plt.title("Loss funtion vs  Training Epoch")
plt.plot(Loss_data)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Trainig Epochs')
plt.axis([0,training_epochs,0,np.max(Loss_data)])
plt.show()

p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average='micro')
print ("F-Score: ", round(f,2))
print ("Precision: ", round(p,2))
print ("Recall: ", round(r,2))


#RETRIEVING THE SAVED MODEL

sess = tf.Session()
new_saver = tf.train.import_meta_graph(r'C:\Users\MA\Desktop\Jazzregmodel.meta')
new_saver.restore(sess, r'/Users/MA/Desktop/Jazzregmodel')
all_vars = tf.get_collection('vars')
for v in all_vars:
    v_ = sess.run(v)
    print(v_)

#TESTINGONUNSEENDATA 

print ("Now, Testing the unlabel data and writing the results")
YPredByNNForUnlabeledData = sess.run(tf.argmax(yPredbyNN,1),feed_dict={X: testing_features})
print (YPredByNNForUnlabeledData)
    
#def load_timestamps():
#def load_dataset(Test_dataset_path):
    
    
for i in range (len(YPredByNNForUnlabeledData)):
        
        if YPredByNNForUnlabeledData[i] == 0:
            ClassLabelFinal.append('reggae')
        
        else:
            ClassLabelFinal.append('jazz')
            
cwd = os.getcwd()
Test_dataset_path = ("/Users/MA/Desktop/TestJR/")
Test_dataset, Total_Instances = load_dataset(Test_dataset_path)
timestamps = load_timestamps(Test_dataset)
write_results('/Users/MA/Desktop/Cls/timestamps', '/Users/MA/Desktop/Cls/ClassLabelFinal', '/Users/MA/Desktop/Cls/Result.csv')
